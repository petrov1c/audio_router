{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "adc28de08c1443fd81cf575eb3e4304c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_946c1c9b726f472697c70f85d775cab5",
              "IPY_MODEL_746149ea3685479290a1a6baf749ebc0",
              "IPY_MODEL_a49b886010f54049b3cc9b3b693e11d4"
            ],
            "layout": "IPY_MODEL_2a675e4bd0fe4c45aa3092c1bca07fe0"
          }
        },
        "946c1c9b726f472697c70f85d775cab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba67822df49449dd8d2a4215713f26a1",
            "placeholder": "​",
            "style": "IPY_MODEL_a5c7dfe7932b42c1923e92c9197f1434",
            "value": "Fetching 10 files: 100%"
          }
        },
        "746149ea3685479290a1a6baf749ebc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c9692c264e447658a6b8e4b68a28861",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60641081105d4366af4995b4d5b0107e",
            "value": 10
          }
        },
        "a49b886010f54049b3cc9b3b693e11d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcf7960210b94ad281b4c45aa448c022",
            "placeholder": "​",
            "style": "IPY_MODEL_da91a30003644ea0842e0912a3897daa",
            "value": " 10/10 [00:00&lt;00:00, 1202.63it/s]"
          }
        },
        "2a675e4bd0fe4c45aa3092c1bca07fe0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba67822df49449dd8d2a4215713f26a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5c7dfe7932b42c1923e92c9197f1434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c9692c264e447658a6b8e4b68a28861": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60641081105d4366af4995b4d5b0107e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fcf7960210b94ad281b4c45aa448c022": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da91a30003644ea0842e0912a3897daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install liquid-audio\n",
        "# ! pip install \"liquid-audio [demo]\" # optional, to install demo dependencies\n",
        "# ! pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zv2-89N8mrmi",
        "outputId": "8b1df39a-0f9c-4692-f106-18e60a6640c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: liquid-audio in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: accelerate>=1.10.1 in /usr/local/lib/python3.12/dist-packages (from liquid-audio) (1.12.0)\n",
            "Requirement already satisfied: einops>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from liquid-audio) (0.8.1)\n",
            "Requirement already satisfied: librosa>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from liquid-audio) (0.11.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from liquid-audio) (0.2.1)\n",
            "Requirement already satisfied: torch>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from liquid-audio) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchaudio>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from liquid-audio) (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers>=4.55.4 in /usr/local/lib/python3.12/dist-packages (from liquid-audio) (4.57.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.10.1->liquid-audio) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.10.1->liquid-audio) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.10.1->liquid-audio) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.10.1->liquid-audio) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.10.1->liquid-audio) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.10.1->liquid-audio) (0.7.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->liquid-audio) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->liquid-audio) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->liquid-audio) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->liquid-audio) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->liquid-audio) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->liquid-audio) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->liquid-audio) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->liquid-audio) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->liquid-audio) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->liquid-audio) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->liquid-audio) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->liquid-audio) (1.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->liquid-audio) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.55.4->liquid-audio) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.55.4->liquid-audio) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.55.4->liquid-audio) (0.22.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.55.4->liquid-audio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.10.1->liquid-audio) (1.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa>=0.11.0->liquid-audio) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa>=0.11.0->liquid-audio) (4.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.55.4->liquid-audio) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.55.4->liquid-audio) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.55.4->liquid-audio) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.55.4->liquid-audio) (2025.11.12)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa>=0.11.0->liquid-audio) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa>=0.11.0->liquid-audio) (2.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.8.0->liquid-audio) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.8.0->liquid-audio) (3.0.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa>=0.11.0->liquid-audio) (2.23)\n",
            "Requirement already satisfied: torchcodec in /usr/local/lib/python3.12/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from liquid_audio import LFM2AudioModel, LFM2AudioProcessor, ChatState, LFMModality\n",
        "\n",
        "# Load models\n",
        "HF_REPO = \"LiquidAI/LFM2-Audio-1.5B\"\n",
        "\n",
        "processor = LFM2AudioProcessor.from_pretrained(HF_REPO).eval()\n",
        "model = LFM2AudioModel.from_pretrained(HF_REPO).eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "adc28de08c1443fd81cf575eb3e4304c",
            "946c1c9b726f472697c70f85d775cab5",
            "746149ea3685479290a1a6baf749ebc0",
            "a49b886010f54049b3cc9b3b693e11d4",
            "2a675e4bd0fe4c45aa3092c1bca07fe0",
            "ba67822df49449dd8d2a4215713f26a1",
            "a5c7dfe7932b42c1923e92c9197f1434",
            "9c9692c264e447658a6b8e4b68a28861",
            "60641081105d4366af4995b4d5b0107e",
            "fcf7960210b94ad281b4c45aa448c022",
            "da91a30003644ea0842e0912a3897daa"
          ]
        },
        "collapsed": true,
        "id": "ih53b094kz2v",
        "outputId": "56f5c97c-502b-4116-d8a1-29d676ab085f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "adc28de08c1443fd81cf575eb3e4304c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Audio\n",
        "from pathlib import Path\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/DLS Speech/local_datasets/weather_dataset_with_tts_v2.tsv\"\n",
        "audio_paths_prefix = \"/content/drive/MyDrive/DLS Speech/\"\n",
        "\n",
        "ds = load_dataset(\n",
        "    \"csv\",\n",
        "    data_files=data_path,\n",
        "    split=\"train\",\n",
        "    column_names=[\"user_text\", \"user_audio\", \"assistant_text\"],  # change to your columns\n",
        "    delimiter=\"\\t\",\n",
        ")\n",
        "\n",
        "\n",
        "def add_audio_path_prefix(example):\n",
        "    p = Path(example[\"user_audio\"])\n",
        "    example[\"user_audio\"] = str(Path(audio_paths_prefix) / p)\n",
        "    return example\n",
        "\n",
        "ds = ds.map(add_audio_path_prefix)\n",
        "\n",
        "\n",
        "ds = ds.map(lambda batch: {\"user_audio_path\": batch[\"user_audio\"]}, batched=True)  # let's copy audio paths to another column\n",
        "ds = ds.cast_column(\"user_audio\", Audio(sampling_rate=16000, decode=True))  # and cast audio column into decoded audio\n",
        "\n",
        "# 90/10 split\n",
        "splits = ds.train_test_split(test_size=0.1, seed=3407, shuffle=True)\n",
        "train_ds = splits[\"train\"]\n",
        "test_ds  = splits[\"test\"]\n",
        "\n",
        "train_ds[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCUyS2dEpbCv",
        "outputId": "6f87ae39-7b8b-4be7-a52b-467ddae96b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user_text': 'Weather update for Phoenix right now, please.',\n",
              " 'user_audio': <datasets.features._torchcodec.AudioDecoder at 0x78a466f1c740>,\n",
              " 'assistant_text': '{\"tool_call\": {\"name\": \"weather.get_forecast\", \"arguments\": {\"city\": \"Phoenix\"}}}',\n",
              " 'user_audio_path': '/content/drive/MyDrive/DLS Speech/local_datasets/audio/000416.wav'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT_JSON = \"\"\"\n",
        "You are a tool-using assistant.\n",
        "You can respond with JSON tool call if the tool usage is required based on the user's request.\n",
        "You need to respond with a tool call if the user asks about weather.\n",
        "If the user asks something else unrelated to weather, tool is not needed, respond to user's query in plain text as usual\n",
        "\n",
        "Available tools (names are exact; use only these names):\n",
        "- weather.get_forecast(city: string, date: YYYY-MM-DD)\n",
        "  - Returns the weather in a given city.\n",
        "\n",
        "Output rules for tool call:\n",
        "- Output format:\n",
        "  {\n",
        "    \"tool_call\": {\n",
        "      \"name\": \"weather.get_forecast\",\n",
        "      \"arguments\": {\n",
        "        \"city\": \"CITY\",\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "- Do not add any extra properties, arrays, comments, or text before/after the JSON.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CCTfsgNTxH3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Testing text-only inputs\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT_V2 = \"\"\"You are a tool-using assistant.\n",
        "You MUST respond with exactly one XML tool call without any surrounding text if the tool usage is required (user is asking about the weather), otherwise respond to user's request in plain text.\n",
        "Template: <tool_call><name>weather.get_forecast</name><arguments><city>CITY</city></arguments></tool_call>. If tool usage is not required answer user's question shortly with plain text\n",
        "\"\"\"\n",
        "\n",
        "# Set up inputs for the model\n",
        "chat = ChatState(processor)\n",
        "\n",
        "chat.new_turn(\"system\")\n",
        "chat.add_text(SYSTEM_PROMPT_V2)\n",
        "chat.end_turn()\n",
        "\n",
        "chat.new_turn(\"user\")\n",
        "chat.add_text(\"What's the current temperature in Voronezh?\")\n",
        "# chat.add_text(\"Is it rainy in Astana?\")\n",
        "# chat.add_text(\"I am trying to understand how CTC training in ASR works. Can you briefly explain that?\")\n",
        "# chat.add_text(\"What is going to happen to me in 6 months if I run 5 km every single day?\")\n",
        "chat.end_turn()\n",
        "\n",
        "# start an assistant turn before feeding inputs to the model\n",
        "chat.new_turn(\"assistant\")"
      ],
      "metadata": {
        "id": "bQLuvZwPuKN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "for t in model.generate_sequential(**chat, max_new_tokens=512):\n",
        "    if t.numel() == 1:\n",
        "        print(processor.text.decode(t), end=\"\", flush=True)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRQ-a7kdu5Xb",
        "outputId": "e2bb3f7b-023a-471a-c86f-94cfe9828b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 5 km every single day for 6 months could lead to several health impacts, both positive and negative. Here are some potential outcomes:\n",
            "\n",
            "1. **Physical Health**: \n",
            "   - **Benefits**: Regular running can improve cardiovascular health, increase muscle strength, enhance flexibility, and aid in weight management.\n",
            "   - **Risks**: Over time, excessive running can lead to injuries such as stress fractures, tendonitis, or muscle strains. It can also cause chronic fatigue and overtraining syndrome.\n",
            "\n",
            "2. **Mental Health**: \n",
            "   - Running can be a great mood booster and help reduce symptoms of depression and anxiety. It can also improve sleep quality and increase energy levels.\n",
            "\n",
            "3. **Lifestyle**: \n",
            "   - Consistent running can lead to a more active lifestyle, encouraging other healthy habits like better nutrition and more physical activity.\n",
            "\n",
            "4. **Time Management**: \n",
            "   - Running 5 km daily requires a significant time commitment, which might impact other aspects of life, such as work, family, or social activities.\n",
            "\n",
            "5. **Environmental Impact**: \n",
            "   - Running can be an eco-friendly activity, but it's important to consider the environmental impact of the gear you use and the energy required to produce it.\n",
            "\n",
            "6. **Long-term Commitment**: \n",
            "   - If you continue this routine, you might develop a strong habit that can be beneficial for your health, but it's important to balance it with rest and recovery to prevent burnout.\n",
            "\n",
            "It's important to listen to your body and ensure you're not pushing yourself too hard. Consulting with a healthcare professional or a fitness expert can provide personalized advice tailored to your specific health needs and goals.<|im_end|>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Testing ASR performance\n",
        "\"\"\"\n",
        "\n",
        "for sample in test_ds.select(range(5)):\n",
        "  user_audio = sample[\"user_audio_path\"]\n",
        "\n",
        "  chat = ChatState(processor)\n",
        "\n",
        "  chat.new_turn(\"system\")\n",
        "  # This model is super sensitive to prompt formulation, better follow example prompts\n",
        "  # https://github.com/Liquid4All/liquid-audio\n",
        "  chat.add_text(\"Perform ASR.\")\n",
        "  chat.end_turn()\n",
        "\n",
        "  chat.new_turn(\"user\")\n",
        "  wav, sampling_rate = torchaudio.load(user_audio)\n",
        "  chat.add_audio(wav, sampling_rate)\n",
        "  chat.end_turn()\n",
        "  chat.new_turn(\"assistant\")\n",
        "\n",
        "  print(f\"Ground truth transcript:\\n{sample['user_text']}\\n\")\n",
        "  print(\"Recognized speech:\")\n",
        "\n",
        "  # Generate text\n",
        "  for t in model.generate_sequential(**chat, max_new_tokens=512):\n",
        "      if t.numel() == 1:\n",
        "          print(processor.text.decode(t), end=\"\", flush=True)\n",
        "  print(\"\\n==================\\n\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRXwfHarzdJ-",
        "outputId": "3c855e3d-cc74-43ae-b442-bfe44cdfe920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth transcript:\n",
            "Is it cold in Granada right now?\n",
            "\n",
            "Recognized speech:\n",
            "Is it cold in Granada right now?<|im_end|>\n",
            "==================\n",
            "\n",
            "Ground truth transcript:\n",
            "Give me the current weather conditions in Miami.\n",
            "\n",
            "Recognized speech:\n",
            "Give me the current weather conditions in Miami.<|im_end|>\n",
            "==================\n",
            "\n",
            "Ground truth transcript:\n",
            "Is it foggy in Melbourne right now?\n",
            "\n",
            "Recognized speech:\n",
            "Is it foggy in Melbourne right now?<|im_end|>\n",
            "==================\n",
            "\n",
            "Ground truth transcript:\n",
            "Is it foggy in Geneva right now?\n",
            "\n",
            "Recognized speech:\n",
            "Is it foggy in Geneva right now?<|im_end|>\n",
            "==================\n",
            "\n",
            "Ground truth transcript:\n",
            "Is it stormy in San Diego at the moment?\n",
            "\n",
            "Recognized speech:\n",
            "Is it stormy in San Diego at the moment?<|im_end|>\n",
            "==================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Maybe the model can do both ASR and tool-call in a single go\n",
        "\"\"\"\n",
        "for sample in test_ds.select(range(5)):\n",
        "  user_audio = sample[\"user_audio_path\"]\n",
        "\n",
        "  chat = ChatState(processor)\n",
        "\n",
        "  chat.new_turn(\"system\")\n",
        "  chat.add_text(f\"{SYSTEM_PROMPT_V2}. Respond with text.\")\n",
        "  chat.end_turn()\n",
        "\n",
        "  chat.new_turn(\"user\")\n",
        "  wav, sampling_rate = torchaudio.load(user_audio)\n",
        "  chat.add_audio(wav, sampling_rate)\n",
        "  chat.end_turn()\n",
        "  chat.new_turn(\"assistant\")\n",
        "\n",
        "  print(f\"Ground truth transcript:\\n{sample['user_text']}\\n\")\n",
        "  print(\"Model reponse:\")\n",
        "\n",
        "  # Generate text\n",
        "  for t in model.generate_sequential(**chat, max_new_tokens=512):\n",
        "      if t.numel() == 1:\n",
        "          print(processor.text.decode(t), end=\"\", flush=True)\n",
        "  print(\"\\n==================\\n\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h76xlEbN1yXH",
        "outputId": "6dd23abd-139e-4cda-d617-f95b8d1b4989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth transcript:\n",
            "Is it cold in Granada right now?\n",
            "\n",
            "Model reponse:\n",
            "I'm sorry, I don't have access to a tool to answer this question. However, I can help you with other types of information. For example, I can provide you with the weather forecast for Granada. Would you like me to check the current weather in Granada?<|im_end|>\n",
            "==================\n",
            "\n",
            "Ground truth transcript:\n",
            "Give me the current weather conditions in Miami.\n",
            "\n",
            "Model reponse:\n",
            "<tool_call>weather.get_forecast</tool_call><arguments><city>Miami</city></arguments></tool_call><|im_end|>\n",
            "==================\n",
            "\n",
            "Ground truth transcript:\n",
            "Is it foggy in Melbourne right now?\n",
            "\n",
            "Model reponse:\n",
            "I don't have real-time data, but you can check the current weather in Melbourne using a weather app or website. Would you like a weather app recommendation?<|im_end|>\n",
            "==================\n",
            "\n",
            "Ground truth transcript:\n",
            "Is it foggy in Geneva right now?\n",
            "\n",
            "Model reponse:\n",
            "I don't have real-time data to check current weather conditions, including fog in Geneva. You might want to check a weather app or website for the latest forecast. Would you like help finding a reliable weather source?<|im_end|>\n",
            "==================\n",
            "\n",
            "Ground truth transcript:\n",
            "Is it stormy in San Diego at the moment?\n",
            "\n",
            "Model reponse:\n",
            "I don't have real-time data, but you can check the current weather in San Diego using a weather app or website. Would you like a weather forecast for San Diego?<|im_end|>\n",
            "==================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}