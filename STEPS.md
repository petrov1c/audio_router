Построение ассистента:
- Ассистент должен принимать голосовой запрос (на русском или английском).
- Понимать запрос напрямую из аудио.
- Генерировать вызов инструмента (функции) в структурированном формате (JSON или XML).
- Опционально выполнять инструмент и получать результат.
- Формировать обоснованный, понятный человеку ответ (возможно, озвученный).

Выбор модели:
- Рекомендуется Gemma-3n E4B Instruct как нативно мультимодальная модель с поддержкой Function Calling. Если модель не справляется с русским языком, может потребоваться дообучение.
- Альтернатива: LFM2-Audio-1.5B (меньше и не поддерживает русский, но может генерировать речь).

Инструменты (Tool(s)):
- Необходимо реализовать хотя бы один инструмент, НО не тот, что дан в примере (не погода)
- Рекомендуется избегать инструментов со сложным парсингом или большим количеством аргументов.

Подготовка данных и стратегия обучения:
- Определение API инструмента: Выбрать простой интерфейс, формат вызова (JSON/XML) и описать схему в системном промпте.

Генерация синтетического текстового датасета:
- Создать 50-100 шаблонов на инструмент, сгенерировать 200–300 текстовых сообщений, определить ожидаемый вызов инструмента, сгенерировать финальный ответ. Добавить 10-20% примеров, не требующих инструмента.
- Синтезировать аудио для каждого текстового запроса (например, с помощью Coqui TTS).

Эксперименты и сравнение пайплайнов:
Сравнить различные рабочие процессы:
(A) Текст → Модель → Вызов инструмента
(B) Аудио → Модель → Транскрипт ASR
(C) Аудио → Модель → Транскрипт + Вызов инструмента за один проход (Joint pipeline)
(D) Аудио → Модель → Транскрипт → Модель → Вызов инструмента (Cascaded pipeline)

Оценить, какой объем дообучения требуется для каждого из них.

Метрики и оценка:
- Корректность вызова инструмента (на тексте): Precision, Recall, False Alarm Rate, Parsable Tool Invocation Rate.
- Разрыв модальностей (Text vs Audio). Измерить разницу в метриках при подаче чистого текста и синтетического аудио.
- Качество ASR: Измерить WER на тестовой выборке.


Неделя 1 – База (промпт-инжиниринг и тестирование)
Загрузка модели.
Тест промпта «вслепую» (zero-shot): аудио «Какая погода завтра в Казани?» → ожидается валидный вызов инструмента (tool_call).
Реализация парсера (обнаружение/валидация JSON; одна попытка повтора в случае некорректного формата).
Создание небольшого текстового набора данных (≥200) и запуск рабочих процессов только с текстом.

Неделя 2 – Данные и первая оценка аудио
Генерация аудио-набора данных с помощью TTS; измерение WER на подмножестве.
Оценка различных рабочих процессов на аудио по сравнению с текстом; регистрация EM вызова инструмента (tool-call EM), доли парсируемых ответов, точности/полноты (precision/recall).

Неделя 3 – Тьюнинг промптов и SFT на аудио
При необходимости: точная настройка модели на текстовых данных (Text SFT) для улучшения форматирования/принятия решения о вызове инструмента.
Повторная оценка метрик использования инструмента (текст и аудио), отчет о приросте и разрыве между модальностями.
Опционально: небольшая точная настройка модели с учетом аудио (audio-conditioned SFT) для улучшения работы на русском языке.
Добавление еще одного инструмента (например, units.convert или nlp.stress).

Неделя 4 – Создание отчета и опционально рабочего процесса n8n
Доработка демонстрационного ноутбука + отчета (результаты, анализ ошибок, выбор наилучшего рабочего процесса).
Опционально: подготовка рабочего процесса n8n для демонстрации.

Креативность выбора инструмента (2 б.)

System-prompt tuning и оценка модели на следование инструкциям (2 б.): чёткая схема инструмента определена; сделана оценка на тест-сете с примерами.

Метрики по текстовому tool-use (пайплайн A) (3 б.): accuracy, precision/recall, false alarm rate, доля ответов моедели, которые удается распарсить; корректная интерпретация результатов.

Дизайн синтетического датасета (4 б.): понятное описание; сбалансированные текст+аудио; пример запросов «без инструмента»;

ASR бенчмаркинг (пайплайн B) (3 б.): WER + короткий разбор ошибок

Бенчмаркинг аудио пайплайнов C–D (3 б.)

Выбор лучшего пайплайна (2 б.): обоснование выбора по метрикам.

Финальный отчёт (3 б.): краткие выводы и фейл-кейсы

